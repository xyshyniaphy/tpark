# Session Summary and Next Steps

## Summary of Current Session

In this session, we focused on improving the HTML cleaning process in the `tokyo_parking_crawler` project.

The following tasks were completed:

1.  **Initial Cleanup of `src/scraper.py`**: The script was cleaned up by removing comments and adding a `clean_html` function to the `WebScraper` class.
2.  **Improved `clean_html` Function**: The `clean_html` function was enhanced to:
    *   Remove `<script>`, `<style>`, and `<meta>` tags.
    *   Remove `class` and `data-*` attributes from all HTML tags.
    *   Remove HTML comments (e.g., `<!-- ... -->`).
3.  **Workflow Integration**: The `clean_html` function was integrated into the main application workflow in `src/workflow.py`. The workflow now cleans the HTML content immediately after scraping and before caching, ensuring that only cleaned HTML is used in subsequent steps.
4.  **Debugging Support**: For debugging purposes, the workflow was temporarily configured to stop after the scraping and cleaning step. A debug file `cleaned_debug.html` is now generated to allow inspection of the cleaned HTML of the first scraped page.

## Next Steps

The next phase of the project is to replace the generic LLM-based data extraction with site-specific parsers for converting cleaned HTML into structured JSON data. This will improve accuracy and reduce reliance on the LLM.

Based on the project's modular architecture defined in `spec.md`, here is the proposed implementation plan:

1.  **Create a Parser Directory**:
    *   A new directory `src/parsers/` will be created to house the site-specific parsers.
    *   An `__init__.py` file in this directory will manage the registration of available parsers.

2.  **Develop Site-Specific Parsers**:
    *   For each target website (e.g., `carparking.jp`), a corresponding parser file will be created (e.g., `src/parsers/carparking_jp.py`).
    *   Each parser will be a class that inherits from a common `BaseParser` interface (to be defined).
    *   The `parse` method of each parser will take the cleaned HTML content (as a BeautifulSoup object) and the URL as input, and will be responsible for extracting the parking lot information and returning a list of `ParkingLot` data models.

3.  **Update the Workflow**:
    *   The `node_extract_with_gemini` in `src/workflow.py` will be replaced with a new node, `node_extract_data`.
    *   This new node will dynamically select the appropriate parser from the `src/parsers/` directory based on the domain of the URL being processed.
    *   If no specific parser is found for a domain, the workflow can be configured to either skip it or fall back to a default (e.g., the Gemini-based extractor).

4.  **Iterative Development**:
    *   We will develop and test one parser at a time, starting with a single HTML file in each session to manage context size and complexity.
    *   The immediate next task is to create the `src/parsers/` directory and the first parser for one of the target websites.
